{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2efbcb3d",
   "metadata": {},
   "source": [
    "Nicholas Brower<br>\n",
    "Springboard DSCT \\[May 2022\\]<br>\n",
    "Unit 26.2.2 (Appendix)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e6444",
   "metadata": {},
   "source": [
    "# Ultimate Technologies Inc. Take-Home Challenge<br>\n",
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a1b3c6",
   "metadata": {},
   "source": [
    "This notebook contains all code used to generate submisisons for unit 26.2.2, the Ultimate Technologies Inc. Take-Home Challege. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ed461",
   "metadata": {},
   "source": [
    "Import required modules and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9f4e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules and packages from standard Python library.\n",
    "from calendar import day_abbr, day_name, firstweekday, month_name, monthrange\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial, reduce\n",
    "from itertools import chain, pairwise\n",
    "import json\n",
    "from operator import methodcaller, attrgetter\n",
    "import re\n",
    "import textwrap\n",
    "from typing import Any, Callable, Iterable, Sequence, Union \n",
    "import warnings\n",
    "\n",
    "# Import third-party modules and packages.\n",
    "import numpy as np\n",
    "from numpy.typing import ArrayLike, DTypeLike\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from matplotlib import cm as colormap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axes import Axes, Subplot\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib.patches as patches\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Import scikitlearn processing and testing options\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.decomposition import (\n",
    "    FactorAnalysis, FastICA, KernelPCA, PCA, SparsePCA, TruncatedSVD\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, GroupKFold, KFold, RandomizedSearchCV, StratifiedKFold, \n",
    "    cross_validate, cross_val_score, train_test_split \n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer, PowerTransformer, QuantileTransformer, MinMaxScaler,\n",
    "    RobustScaler, StandardScaler\n",
    ")\n",
    "\n",
    "# Import scikitlearn metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, auc, balanced_accuracy_score, f1_score,\n",
    "    mean_absolute_percentage_error as mape, mean_absolute_error as mae, \n",
    "    mean_squared_error as mse, median_absolute_error as median_abs_error,\n",
    "    precision_score, r2_score, recall_score, classification_report\n",
    ")\n",
    "rmse = partial(mse, squared=False)\n",
    "\n",
    "# Import scikitlearn classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, HistGradientBoostingClassifier, RandomForestClassifier,\n",
    ")\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import ComplementNB, GaussianNB, MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
    "\n",
    "# Import scikitlearn regressors\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostRegressor, HistGradientBoostingRegressor\n",
    ")\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import LinearSVR, SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6edc854",
   "metadata": {},
   "source": [
    "Establish constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4423c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARIMA, SARIMAX = sm.tsa.ARIMA, sm.tsa.SARIMAX\n",
    "DATA = 'Data/'\n",
    "RAW_DATA = f'{DATA}/Raw/'\n",
    "HR = 79 * '-'\n",
    "BR = '\\n'\n",
    "TAB = '    '\n",
    "WEEKDAYS = list(day_name)\n",
    "WEEKDAYS_ABBR = list(day_abbr)\n",
    "WEEKDAY_MAP = dict(enumerate(WEEKDAYS))\n",
    "WEEKDAY_ABBR_MAP = dict(enumerate(WEEKDAYS_ABBR))\n",
    "MONTH_MAP = {i: month_name for i, month_name in enumerate(month_name) if i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f9b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_by_num_null(\n",
    "        data: pd.DataFrame, threshold: int=2, verbose: bool=False, \n",
    "        inplace: bool=False) -> Union[None, pd.DataFrame]:\n",
    "    '''Drop records where the number of null fields is greater than or \n",
    "    equal to a given threshold.\n",
    "    '''\n",
    "    df = data.copy()\n",
    "    n_or_more_missing = df.loc[df.isna().sum(axis=1) >= threshold].index\n",
    "    if verbose:\n",
    "        num_dropped = len(missing_2_or_more_fields)\n",
    "        print(\n",
    "            f'{num_dropped:,.0f} records dropped '\n",
    "            + f'({100*num_dropped/len(ultimate):.2f} %)'\n",
    "        )\n",
    "        del num_dropped\n",
    "    if inplace:\n",
    "        data = df\n",
    "    else:\n",
    "        return df\n",
    "        \n",
    "def composite_function(*funcs: Callable) -> Callable:\n",
    "    '''Return a composite function composed from an ordered sequence of \n",
    "    functions.\n",
    "    '''\n",
    "    return (\n",
    "        lambda initial: reduce(\n",
    "            lambda state, func: func(state), funcs, initial\n",
    "        )\n",
    "    )\n",
    "    \n",
    "def series_color_array(data: pd.Series, color: Union[ArrayLike, Sequence]):\n",
    "    '''Return an array of shape (len(data), len(color)) comprised of the\n",
    "    array passed to color repeated len(data) times.\n",
    "    '''\n",
    "    return np.full((len(data), len(color)), np.array(color))\n",
    "\n",
    "def series_color(\n",
    "        data: pd.Series, color: Union[Sequence, ArrayLike], \n",
    "        func: Union[Callable, str], *args, **kwargs) -> ArrayLike:\n",
    "    if isinstance(func, str):\n",
    "        func = methodcaller(func, *args, **kwargs)\n",
    "    else:\n",
    "        func = partial(func, *args, **kwargs)\n",
    "    color = np.array(color)\n",
    "    colors = series_color_array(data, color)\n",
    "    return func(\n",
    "        np.tile(data.values.reshape(-1, 1), reps=color.shape[-1]),\n",
    "        color\n",
    "    )\n",
    "\n",
    "def first_where(\n",
    "        iterable: Iterable, condition: Union[Callable, None]=None, \n",
    "        default: Union[Any, None]=None\n",
    ") -> Any:\n",
    "    '''Return the first element in an iterable for which \n",
    "    condition(element) returns True.\n",
    "    \n",
    "    Parameters:\n",
    "    ====================================================================\n",
    "    iterable: Iterable\n",
    "        An iterable object compatible with the built-in filter function \n",
    "        and the Callable passed to the condition parameter of this\n",
    "        function.\n",
    "    condition: Union[Callable, None], default=None\n",
    "        A Callable passed to the first positional argument of the\n",
    "        built-in filter function.\n",
    "    default: Union[Any, None], default=None\n",
    "        A default value passed to the last positional argument of the \n",
    "        built-in next function. This value is returned if not any\n",
    "        condition(element) is True.\n",
    "    ====================================================================\n",
    "    \n",
    "    Returns: Union[Any, None]\n",
    "    The first element for which condition(element) is True or the\n",
    "    value specified in the default argument.\n",
    "    '''\n",
    "    return next(filter(condition, iterable), default)\n",
    "\n",
    "def is_none(obj: Any) -> bool:\n",
    "    '''Return the evaluated boolean expression 'obj is None'.\n",
    "    '''\n",
    "    return obj is None\n",
    "\n",
    "def is_not_none(obj: Any) -> bool:\n",
    "    '''Return the evaluated boolean expression 'obj is not None'.\n",
    "    '''\n",
    "    return obj is not None\n",
    "\n",
    "def is_null(obj: Any) -> Any:\n",
    "    '''Return the result of passing obj to the Pandas isnull function.\n",
    "    '''\n",
    "    py_type = firstwhere([list, set, tuple], partial(isinstance, obj), False)\n",
    "    if py_type:\n",
    "        return py_type(map(pd.isnull, obj))\n",
    "    return ps.isnull(obj)\n",
    "\n",
    "def not_null(obj: Any) -> Any:\n",
    "    '''Return the result of passing obj to the Pandas notnull function.\n",
    "    '''\n",
    "    py_type = firstwhere([list, set, tuple], partial(isinstance, obj), False)\n",
    "    if py_type:\n",
    "        return py_type(map(pd.notnull, obj))\n",
    "    return pd.notnull(obj)\n",
    "\n",
    "def first_not_none(iterable, default: Any=False) -> Any:\n",
    "    '''Return the first element of an iterable that is not None, or a\n",
    "    default if all elements are None.\n",
    "    \n",
    "    Parameters:\n",
    "    ====================================================================\n",
    "    iterable: Iterable\n",
    "        An iterable object compatabile with the built-in filter function\n",
    "        and the locally-defined is_not_none function.\n",
    "    default: Any, default=False\n",
    "        Returned if all elements of iterable are None.\n",
    "    ====================================================================\n",
    "    Returns: Any\n",
    "    '''\n",
    "    return first_where(iterable, condition=is_not_none, default=default)\n",
    "\n",
    "def indented(\n",
    "        text: str, level: int=1, lw: Union[int, None]=79, \n",
    "        as_spaces: bool=True, initial: Union[int, None]=None, \n",
    "        subsequent: Union[int, None]=None, spaces_per_tab: int=4\n",
    ") -> str:\n",
    "    '''Return an indented, text-wrapped string.\n",
    "    \n",
    "    Parameters\n",
    "    ====================================================================\n",
    "    text: str\n",
    "        A string containing text to be processed.\n",
    "    level: int, default=1\n",
    "        An integer indicating the desired indentation level.\n",
    "    lw: Union[int, None], default=79\n",
    "        An integer specifying the desired line width or None. If None,\n",
    "        the returned text is not wrapped\n",
    "    \n",
    "    Returns: str\n",
    "    '''\n",
    "    _i, _s = [initial, level], [subsequent, level]\n",
    "    _i, _s = map(\n",
    "        partial(first_not_none, default=level), [_i, _s]\n",
    "    )\n",
    "    _tab = spaces_per_tab * ' ' if as_spaces else '\\t'\n",
    "    _i, _s = _i * _tab, _s * _tab\n",
    "    if lw is None:\n",
    "        return _i + re.sub(r'(\\n|\\r)', fr'\\1{_s}', text)\n",
    "    return '\\n'.join([\n",
    "        '\\n'.join(\n",
    "            textwrap.wrap(\n",
    "                text_part, width=lw, initial_indent=_i, subsequent_indent=_s,\n",
    "                drop_whitespace=False\n",
    "            )\n",
    "        ) \n",
    "        for text_part in re.split('\\n', re.sub(r'\\r', r'\\n', text))\n",
    "    ])\n",
    "def hr(\n",
    "        characters: str='-', lw: int=79, br: bool=False, indent_level: int=0,\n",
    "        indent_str: str=TAB, newline_character: str='\\n'\n",
    ") -> str:\n",
    "    '''Return a string depicting a horizontal rule comprised of a given \n",
    "    a character or characters repeated such that the resultant length\n",
    "    of the string is equal to a given line width.\n",
    "    \n",
    "    Parameters\n",
    "    characters: str, default='-'\n",
    "        The character(s) to repeat.\n",
    "    lw: int, default=79\n",
    "        An integer specifying the desired length of the string.\n",
    "    br: bool, default=False\n",
    "        If true, return a horizontal rule followed by the newline\n",
    "        character '\\n'.\n",
    "    indent_level: int, default=0\n",
    "        Specifies the indent level at which the horizontal rule begins.\n",
    "    indent_str: str, default='    '\n",
    "        A string specifying the indent type. Default is 4 spaces.\n",
    "    newline_character: str, default='\\n'\n",
    "        A string containing the newline character used when br is True.\n",
    "        \n",
    "    Returns: str\n",
    "    '''\n",
    "    indent = indent_level * indent_str\n",
    "    lw = lw - len(indent)\n",
    "    if lw <= 0:\n",
    "        return indent + int(br) * newline_character\n",
    "    rule = ((lw//len(characters) + 1) * characters)[:lw]\n",
    "    return indent + rule + int(br) * newline_character\n",
    "\n",
    "def month_length_from_datetime(dt: datetime) -> int:\n",
    "    '''Return the length of a month in days from a given datetime.\n",
    "    '''\n",
    "    return monthrange(dt.year, dt.month)[1]\n",
    "def datetime_less_parts(\n",
    "        dt: datetime, keep: list[str]=['year', 'month', 'day']) -> datetime:\n",
    "    date = dt.date()\n",
    "    return datetime(\n",
    "        **{\n",
    "            attr: getattr(date, attr) if attr in keep else 0\n",
    "            for attr in ['year', 'month', 'day']\n",
    "        }, **{\n",
    "            attr: getattr(dt, attr) if attr in keep else 0\n",
    "            for attr in ['hour', 'minute', 'second']\n",
    "        }, **{\n",
    "            attr: getattr(dt, attr) if attr in keep else 0\n",
    "            for attr in ['microsecond', 'nanosecond'] if attr in keep\n",
    "        }\n",
    "        \n",
    "    )\n",
    "def datetime_adjust_resolution(dt: datetime, resolution: str) -> datetime:\n",
    "    parts = [\n",
    "        'year', 'month', 'day', 'hour', 'minute', 'second', 'microsecond', \n",
    "        'nanosecond'\n",
    "    ]\n",
    "    defaults = [1970, 1, 1, 0, 0, 0, 0]\n",
    "    date = dt.date()\n",
    "    dt_parts = [\n",
    "        *[getattr(date, _) for _ in parts[:3]], \n",
    "        *[getattr(dt, _) for _ in parts[3:]]\n",
    "    ]\n",
    "    res = parts.index(resolution) + 1\n",
    "    return datetime(**dict(zip(parts, [*dt_parts[:res], *defaults[res:]])))\n",
    "    \n",
    "    \n",
    "def datetime_floor(\n",
    "        dt: datetime, resolution: DateOffset, \n",
    "        reference:Union[datetime, None]=None) -> datetime:\n",
    "    '''Return a datetime rounded down to the nearest multiple of a given \n",
    "    resolution.\n",
    "    '''\n",
    "    _date = dt.date()\n",
    "    if reference is None:\n",
    "        reference = datetime(\n",
    "            **{_: getattr(_date, _) for _ in ['year', 'month', 'day']},\n",
    "            **{_: 0 for _ in ['hour', 'minute', 'second']}\n",
    "        )   \n",
    "    dt_range = pd.date_range(\n",
    "        reference, reference + DateOffset(days=1), freq=resolution\n",
    "    )\n",
    "    return dt_range[(dt_range < dt)].max()\n",
    "\n",
    "def outliers(\n",
    "        data: pd.Series, \n",
    "        reference: {'mean', 'median', 'quantile', 'quantiles'}='quantile',\n",
    "        threshold_type: {'fixed', 'multiple'}='multiple', threshold: float=1.5,\n",
    "        threshold_multiple: {'std', 'var', 'iqr'}='iqr', \n",
    "        return_type: Union[bool, str, 'extent', 'ratio', list]=str,\n",
    "        ) -> Union[pd.Series, pd.DataFrame]:\n",
    "    '''Return a series describing values in a given series in terms of\n",
    "    their relationship to the distribution of all values in the Series.\n",
    "    \n",
    "    Parameters\n",
    "    data: pd.Series\n",
    "        A Pandas Series containing numeric data.\n",
    "    reference: (\n",
    "            {'mean', 'median', quantile', 'quantiles'}, \n",
    "            default='quantile'\n",
    "    )\n",
    "        A string in the set depicted above. If 'mean' or 'median',\n",
    "        outliers are defined as values whose absolute difference from \n",
    "        the given aggregate of the provided data is greater than a \n",
    "        threshold. If 'quantile' or 'quantiles', low and high outliers \n",
    "        are classified as those less than the 25th percentile minus a \n",
    "        threshold or greater than the 75th percentile plus a threshold.\n",
    "    threshold_type: {'fixed', 'multiple'}, default='multiple'\n",
    "        A string in the set depicted above. If fixed, outliers are \n",
    "        defined as any values in data that exist outside of the open \n",
    "        interval bound by the reference value(s) ± the value passed to \n",
    "        the threshold parameter. If 'multiple', a 'fixed' threshold\n",
    "        magnitude is calculated using the statistic name passed to the \n",
    "        threshold_multiple parameter, such that\n",
    "        magnitude = threshold * (statistic of data).\n",
    "    threshold: float, default=1.5\n",
    "        A float indicating a threshold as described in threshold_type.\n",
    "    threshold_multiple: {'std', 'var' 'iqr'}, default='iqr'\n",
    "        A string of the set depicted above, indicating the type of \n",
    "        calculations used to establish the thresholds beyond which \n",
    "        values in data are classified as outliers. Only relevant when \n",
    "        'multiple' is passed as argument to the threshold_type\n",
    "        parameter.  If threshold_multiple is 'std', and reference is\n",
    "        'mean', outliers are defined as any values in data whose \n",
    "        absolute difference from the mean is greater than x times the \n",
    "        standard deviation of data, where x equals the value passed to \n",
    "        threshold.\n",
    "    return_type: (\n",
    "            Union[{bool, str, 'extent', 'ratio'}, list], default=str\n",
    "        )\n",
    "        One of the types or strings depicted in the set above or a list\n",
    "        thereof. Determines the datatype of the returned Series or \n",
    "        datatypes of the returned DataFrame.\n",
    "        \n",
    "        bool\n",
    "            Return a boolean Series indicating whether each value is an\n",
    "            outlier.\n",
    "        str\n",
    "            Return a Series of strings indicating the type of outlier of\n",
    "            each value. A value in data deemed an outlier is described \n",
    "            as 'high' or 'low' at its index in the returned Series. The \n",
    "            returned series is null at the index of all non-outliers in\n",
    "            data.\n",
    "        extent\n",
    "            Return a Series of floating point values indicating the\n",
    "            difference between the value of each outlier and the \n",
    "            threshold it exceeded to warrant its classification. The\n",
    "            returned Series is null at the index of any value in data\n",
    "            not classified as an outlier.\n",
    "        ratio\n",
    "            Return a Series of floating point values describing the\n",
    "            difference between the value of each outlier and the \n",
    "            threshold it exceeded to warrant its classification in terms \n",
    "            of the reference value used to set the threshold. When \n",
    "            outliers are defined relative to the mean using a threshold \n",
    "            set as a multiple of the standard deviation, the returned \n",
    "            Series indicates the distance of each outlier from the mean \n",
    "            as a multiple of the standard deviation. (A value of -2 \n",
    "            indicates a low outlier with a value equal to the mean of \n",
    "            data minus 2 times the standard deviation of data)\n",
    "    \n",
    "    Returns: pd.Series | pd.DataFrame\n",
    "        A Series or DataFrame as described in return_type.\n",
    "    '''\n",
    "    if 'quantile' in reference:\n",
    "        references = data.quantile([0.25, 0.75])\n",
    "    else:\n",
    "        references = data.agg(2 * [reference])\n",
    "    if threshold_type == 'fixed':\n",
    "        thresh_ratio_unit, thresh_magnitude = threshold, threshold\n",
    "        thresholds = references + (np.array([-1, 1]) * threshold)\n",
    "        ratio_unit_name = f'scalar: {threshold})'\n",
    "        ratio_base_multiple = 1\n",
    "    else:\n",
    "        if threshold_multiple == 'iqr':\n",
    "            thresh_ratio_unit = np.subtract(*data.quantile([0.75, 0.25]))            \n",
    "        else:\n",
    "            thresh_ratio_unit = methodcaller(threshold_multiple)(data)\n",
    "        thresh_magnitude = threshold * thresh_ratio_unit\n",
    "        ratio_base_multiple = threshold\n",
    "        ratio_unit_name = threshold_multiple\n",
    "        thresholds = references + (np.array([-1, 1]) * thresh_magnitude)\n",
    "        \n",
    "    low_thresh, hi_thresh = thresholds\n",
    "    \n",
    "    def _is_outlier(value: float) -> bool:\n",
    "        '''Return a boolean indicating whether a given value is either\n",
    "        greater than a high threshold or less than a low threshold.\n",
    "        '''\n",
    "        return value < low_thresh or value > hi_thresh\n",
    "    def _outlier_type(\n",
    "            value: float, outlier_extent: Union[float, None]=None) -> str:\n",
    "        '''Return a string indicating whether an outlier is high or low.\n",
    "        '''\n",
    "        if outlier_extent is not None:\n",
    "            return {-1: 'low', 0: np.nan, 1: 'high'}[np.sign(outlier_extent)]\n",
    "        elif value < low_thresh:\n",
    "            return 'low'\n",
    "        elif value > hi_thresh:\n",
    "            return 'high'\n",
    "    def _outlier_extent(\n",
    "            value: float, outlier_type: Union[str, None]=None) -> float:\n",
    "        '''Return the amount by which a given value exceeds the\n",
    "        threshold by which it was classified as an outlier. If the value \n",
    "        is not an outlier or the value is null, return None.\n",
    "        '''\n",
    "        if pd.notna(outlier_type):\n",
    "            if outlier_type=='low':\n",
    "                return -1 * (low_thresh - value)\n",
    "            elif outlier_type=='high':\n",
    "                return value - hi_thresh\n",
    "        elif pd.notna(value):\n",
    "            if value < low_thresh:\n",
    "                return -1 * (low_thresh - value)\n",
    "            elif value > hi_thresh:\n",
    "                return value - hi_thresh\n",
    "    def _outlier_ratio(\n",
    "            value: float, outlier_extent: Union[float, None]=None) -> str:\n",
    "        '''Return the multiple of a base quantity by which an outlier\n",
    "        exceeds a threshold.\n",
    "        '''\n",
    "        if pd.isna(outlier_extent):\n",
    "            outlier_extent = _outlier_extent(value)\n",
    "        if pd.notna(outlier_extent):\n",
    "            return (\n",
    "                np.sign(outlier_extent)\n",
    "                * (abs(outlier_extent/thresh_ratio_unit) + ratio_base_multiple)\n",
    "            )\n",
    "\n",
    "    \n",
    "    _out_funcs = {\n",
    "        bool: _is_outlier, str: _outlier_type, 'extent': _outlier_extent,\n",
    "        'ratio': _outlier_ratio\n",
    "    }\n",
    "    _func_names = {\n",
    "        f'_{name}': name \n",
    "        for name in [\n",
    "            'is_outlier', 'outlier_type', 'outlier_extent', 'outlier_ratio'\n",
    "        ]\n",
    "    }\n",
    "    _col_names = dict(zip(_out_funcs.keys(), _func_names.values()))\n",
    "    \n",
    "    if isinstance(return_type, list):\n",
    "        funcs = [_out_funcs[rt] for rt in return_type]\n",
    "        outliers = data.copy().apply(funcs).rename(columns=_func_names)\n",
    "        outliers.attrs['name'] = (\n",
    "            f'{data.name}{hasattr(data, \"name\")*\"_\"}outliers)'\n",
    "        )\n",
    "        outliers.rename(\n",
    "            columns={'outlier_ratio': f'outlier_ratio_{ratio_unit_name}'},\n",
    "            inplace=True\n",
    "        )\n",
    "        return outliers\n",
    "    else:\n",
    "        func, name = _out_funcs[return_type], _func_names[return_type]\n",
    "        if return_type == 'ratio':\n",
    "            name = f'outlier_ratio_{ratio_unit_name}'\n",
    "        return pd.Series(\n",
    "            data=data.apply(func).values, index=data.index,\n",
    "            name=(f'{data.name}{any(data.name) * \"_\"}{name}')\n",
    "        )\n",
    "\n",
    "def quantile_boundaries(\n",
    "            data: pd.DataFrame, \n",
    "            quantiles: ArrayLike=[0.25, 0.5, 0.75] ) -> pd.DataFrame:\n",
    "    '''Return a DataFrame comprised of the minimum, each quantile value \n",
    "    in a given array, and the maximum of each column in data.\n",
    "    '''\n",
    "    names = [\n",
    "        '_min', \n",
    "        *[f'{i:.0f}th_ptile' for i in [100*_ for _ in quantiles]], \n",
    "        '_max'\n",
    "    ]\n",
    "    aggs = [\n",
    "        'min', *[methodcaller('quantile', _) for _ in quantiles], 'max'\n",
    "    ]\n",
    "    results = pd.DataFrame(index=names, columns=data.columns)\n",
    "    for name, agg in zip(names, aggs):\n",
    "        results.loc[name] = data.agg(agg)\n",
    "    return results\n",
    "def quantile_groups(\n",
    "            data: pd.DataFrame, \n",
    "            quantiles: ArrayLike=[0.25, 0.5, 0.75]) -> pd.DataFrame:\n",
    "    '''Return a DataFrame of integers indicating the quantile group of \n",
    "    each value in each column of data.\n",
    "    '''\n",
    "    q_boundaries = quantile_boundaries(data, quantiles)\n",
    "    boundary_pairs = {\n",
    "        col: list(pairwise(q_boundaries.loc[:, col].values))\n",
    "        for col in q_boundaries.columns\n",
    "    }\n",
    "    for col, pairs in boundary_pairs.items():\n",
    "        for pair, count in {pair: pairs.count(pair) for pair in pairs}.items():\n",
    "            for i in range(count-1):\n",
    "                pairs.remove(pair)\n",
    "        for col, pairs in boundary_pairs.items():\n",
    "            assert pairs[0][0] == q_boundaries.loc['_min', col]\n",
    "    include = {col: [] for col in boundary_pairs.keys()}\n",
    "    for col, pairs in boundary_pairs.items():\n",
    "        last = 'left'\n",
    "        for pair in pairs:\n",
    "            if pair[0] == pair[1]:\n",
    "                include[col].append('both')\n",
    "                last = 'both'\n",
    "            elif last == 'both':\n",
    "                if pairs.index(pair)==len(pairs)-1:\n",
    "                    include[col].append('right')\n",
    "                else:\n",
    "                    include[col].append('neither')\n",
    "                    last = 'neither'\n",
    "            elif pairs.index(pair)==len(pairs)-1:\n",
    "                include[col].append('both')\n",
    "            else:\n",
    "                include[col].append('left')\n",
    "    columns = [f'{col}_q' for col in data.columns]\n",
    "    results = pd.DataFrame(index=data.index, columns=columns, dtype=int)\n",
    "    for col, pairs in boundary_pairs.items():\n",
    "        inclusive = include[col]\n",
    "        assert len(pairs)==len(inclusive)\n",
    "        for i, ((low, hi), inc) in enumerate(zip(pairs, inclusive), 1):\n",
    "            results.loc[data.index[data[col].between(low, hi, inclusive=inc)], f'{col}_q'] = i\n",
    "    return results\n",
    "\n",
    "def kfcv_scores(\n",
    "        X, y, scorers: list[Callable], model: BaseEstimator,\n",
    "        names: Union[list[str], None]=None, *args, **kwargs) -> dict:\n",
    "    scores = {_: [] for _ in scorers}\n",
    "    kf = KFold(*args, **kwargs)\n",
    "    for train, test in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train], X.iloc[test]\n",
    "        y_train, y_test = y.iloc[train], y.iloc[test]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_p = model.predict(X_test)\n",
    "        for func, score_list in scores.items():\n",
    "            score_list.append(func(y_test, y_p))\n",
    "    scores = {k: np.mean(v) for k, v in scores.items()}\n",
    "    if names is not None:\n",
    "        return dict(zip(names, scores.values()))\n",
    "    return scores\n",
    "    \n",
    "    \n",
    "def is_weekend(dt: datetime) -> bool:\n",
    "    '''Return a boolean indicating whether the day of the week of a \n",
    "    given datetime is Saturday or Sunday.\n",
    "    '''\n",
    "    return datetime.weekday(dt) >= 5\n",
    "\n",
    "def is_weekday(dt: datetime) -> bool:\n",
    "    '''Return a boolean indicating whether the day of the week of a \n",
    "    given datetime is not Saturday or Sunday.\n",
    "    '''\n",
    "    return datetime.weekday(dt) < 5\n",
    "    \n",
    "def clock_degrees(\n",
    "            hour: Union[int, None]=None, minute: Union[int, None]=None,\n",
    "            second: Union[float, int, None]=None,\n",
    "            time: Union[datetime, None]=None) -> float:\n",
    "    '''Return the location on a circle in degrees for the time attribute\n",
    "    of a given datetime instance or a sequence of time components.\n",
    "    '''\n",
    "    if time is not None:\n",
    "        hour, minute, second = (\n",
    "            time.hour, time.minute,\n",
    "            time.second + time.nanosecond\n",
    "        )\n",
    "    hour = hour%12\n",
    "    minute, second = minute or 0, second or 0\n",
    "    time = hour + minute/60 + second/3600\n",
    "    time = time/12 * 360\n",
    "    return time\n",
    "    \n",
    "def clock_radians(\n",
    "            hour: Union[int, None]=None, minute: Union[int, None]=None,\n",
    "            second: Union[float, int, None]=None,\n",
    "            time: Union[datetime, None]=None) -> float:\n",
    "    '''Return the location on a circle in radians for the time attribute\n",
    "    of a given datetime instance or a sequence of time components.\n",
    "    '''\n",
    "    if time is not None:\n",
    "        hour, minute, second = (\n",
    "            time.hour, time.minute,\n",
    "            time.second + time.nanosecond\n",
    "        )\n",
    "    hour = hour%12\n",
    "    minute, second = minute or 0, second or 0\n",
    "    time = hour + minute/60 + second/3600\n",
    "    time = time/12 * 360\n",
    "    return np.deg2rad(time)\n",
    "\n",
    "def add_clock_subplot(\n",
    "        fig: Figure, rect: Union[tuple, None]=None, \n",
    "        fontsize: Union[int, None]=None) -> Axes:\n",
    "    '''Add a polar subplot to a given Figure instance with markings and \n",
    "    text representing an analog clock.\n",
    "    '''\n",
    "    if rect is None:\n",
    "        figwidth, figheight = fig.get_figwidth(), fig.get_figheight()\n",
    "        dims = np.array([figheight, figwidth])\n",
    "        dims = min(dims/dims.max())\n",
    "        rect = (1, 0, dims, dims)\n",
    "        del figwidth, figheight, dims\n",
    "    if fontsize is None:\n",
    "        fontsize = fig.get_figheight() * 4\n",
    "    clock = fig.add_axes(\n",
    "        rect, projection='polar', aspect='equal', theta_direction=-1, \n",
    "        theta_offset=(np.pi/2)\n",
    "    )\n",
    "    clock.grid(visible=False)\n",
    "    clock.set_yticks([], []); clock.set_xticks([], [])\n",
    "    clock.set_ylim((0, 1))\n",
    "    clock_hours = [12, *range(1, 12)]\n",
    "    for hour in clock_hours:\n",
    "        clock.plot(\n",
    "            2*[clock_radians(hour)], [0.95, 0.9], color=(0, 0, 0), lw=0.8\n",
    "        )\n",
    "        clock.text(\n",
    "            x=clock_radians(hour), y=0.7, s=f'{hour:.0f}', va='center', \n",
    "            ha='center', fontsize=fontsize\n",
    "        )\n",
    "        for minute in np.linspace(0, 60, 5, endpoint=False):\n",
    "            clock.plot(\n",
    "                2*[clock_radians(hour=hour, minute=minute)], \n",
    "                [0.925, 0.9125], color=(0, 0, 0, 0.5), lw=0.5\n",
    "            )\n",
    "    clock.bar(0, width=np.deg2rad(360), height=0.025, color='black')\n",
    "    return clock\n",
    "\n",
    "\n",
    "    \n",
    "def plt_gca_set_title(*args, **kwargs) -> None:\n",
    "    '''Set the plot title on a current figure's axes object.\n",
    "    '''\n",
    "    if 'ax' in kwargs:\n",
    "        ax = kwargs.pop('ax')\n",
    "    else:\n",
    "        ax = plt.gca()\n",
    "    ax.set_title(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fda93d",
   "metadata": {},
   "source": [
    "Create a convenience function to set plot titles with consistent parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "909aaaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_plot_title = partial(plt_gca_set_title, pad=10, x=0, loc='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc8e206",
   "metadata": {},
   "source": [
    "Create a convenience function to return a dictionary of K-fold cross-validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0265351",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfcv = partial(\n",
    "    kfcv_scores, scorers=[rmse, r2_score, mape], names=[\n",
    "        'RMSE', 'R2', \"Mean absolute % error\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82bdcd4",
   "metadata": {},
   "source": [
    "Define transformer classes compatible with sklearn's Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9fa1974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameColumnTransformer(BaseEstimator):\n",
    "    def __init__(\n",
    "            self, column: str, func: Union[Callable, None, str], args: list=[], \n",
    "            kwargs: dict={}):\n",
    "        self.func = (\n",
    "            methodcaller(func, *args, **kwargs) if isinstance(func, str)\n",
    "            else lambda X: func(X, *args, **kwargs)\n",
    "        )\n",
    "        self.column = column\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "    def fit(self, X: ArrayLike, y: Union[ArrayLike, None]=None):\n",
    "        return self\n",
    "    def transform(\n",
    "            self, X: ArrayLike, y: Union[ArrayLike, None]=None) -> ArrayLike:\n",
    "        X[self.column] = self.func(X[self.column])\n",
    "        return X\n",
    "    def fit_transform(\n",
    "            self, X: ArrayLike, y: Union[ArrayLike, None]=None) -> ArrayLike:\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "class AddDerivedFeatureTransformer(BaseEstimator):\n",
    "    def __init__(\n",
    "            self, input_feature: str, derived_feature_name: str, \n",
    "            func: Callable, output_dtype: Union[str, DTypeLike, None]=None, \n",
    "            retain_derived_feature_values: bool=False, \n",
    "            drop_input_feature: bool=False):\n",
    "        self.input_feature = input_feature\n",
    "        self.derived_feature_name = derived_feature_name\n",
    "        self.retain_derived_feature_values = retain_derived_feature_values\n",
    "        self.func = func\n",
    "        self.output_dtype = output_dtype\n",
    "        self.drop_input_feature = drop_input_feature\n",
    "    def fit(self, X: ArrayLike, y: Union[ArrayLike, None]=None):\n",
    "        if self.retain_derived_feature_values:\n",
    "            self.derived_feature_values = self.func(\n",
    "                X[self.input_feature].copy()\n",
    "            )\n",
    "        else:\n",
    "            self.derived_feature_values = None\n",
    "        return self\n",
    "    def transform(\n",
    "            self, X: ArrayLike, y: Union[ArrayLike, None]=None) -> ArrayLike:\n",
    "        if self.derived_feature_values is not None:\n",
    "            X[self.derived_feature_name] = self.derived_feature_values\n",
    "        else:\n",
    "            X[self.derived_feature_name] = self.func(X[self.input_feature])\n",
    "            if self.output_dtype is not None:\n",
    "                X[self.derived_feature_name] = (\n",
    "                    X[self.derived_feature_name].astype(self.output_dtype)\n",
    "                )\n",
    "        if self.drop_input_feature:\n",
    "            X.drop(columns=[self.input_feature], inplace=True)\n",
    "        return X\n",
    "    def fit_transform(\n",
    "            self, X: ArrayLike, y: Union[ArrayLike, None]=None) -> ArrayLike:\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "class AddDummiesTransformer(BaseEstimator):\n",
    "    def __init__(\n",
    "            self, input_feature: str, values: Union[list, dict, None]=None,\n",
    "            names: Union[list, None]=None, drop_one: bool=False, \n",
    "            drop_input_feature: bool=False):\n",
    "        self.input_feature = input_feature\n",
    "        if isinstance(values, dict):\n",
    "            self.values = [*values.keys()]\n",
    "            self.names = [*values.values()]\n",
    "        else:\n",
    "            self.values = values\n",
    "            self.names = names\n",
    "        self.drop_one = drop_one\n",
    "        self.drop_input_feature = drop_input_feature\n",
    "    def fit(self, X: ArrayLike, y: Union[ArrayLike, None]=None):\n",
    "        if self.values is None:\n",
    "            self.values = X[self.input_feature].unique()\n",
    "            if self.drop_one:\n",
    "                if pd.Series(self.values).isna().any():\n",
    "                    self.values = pd.Series(self.values).dropna().values\n",
    "                else:\n",
    "                    self.values = self.values[:-1]\n",
    "        if self.names is None:\n",
    "            if pd.api.types.is_numeric_dtype(X[self.input_feature]):\n",
    "                self.names = [f'{self.input_feature}_{v}' for v in self.values]\n",
    "            elif pd.api.types.is_bool_dtype(X[self.input_feature]):\n",
    "                self.names = [\n",
    "                    f'{self.input_feature}_{int(v):.0f}'\n",
    "                    if not pd.isna(v) else f'{self.input_feature}_is_null'\n",
    "                    for v in self.values\n",
    "                ]\n",
    "            else:\n",
    "                self.names = self.values\n",
    "        return self\n",
    "    def transform(\n",
    "            self, X: ArrayLike, y: Union[ArrayLike, None]=None) -> ArrayLike:\n",
    "        for value, name in zip(self.values, self.names):\n",
    "            if not pd.isna(value):\n",
    "                X[name] = (X[self.input_feature] == value).astype(int)\n",
    "            else:\n",
    "                X[name] = X[self.input_feature].isna().astype(int)\n",
    "        if self.drop_input_feature:\n",
    "            return X.drop(columns=[self.input_feature])\n",
    "        return X\n",
    "    def fit_transform(\n",
    "            self, X: ArrayLike, y: Union[ArrayLike, None]=None) -> ArrayLike:\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "class DropNullRecordsTransformer(BaseEstimator):\n",
    "    def __init__(\n",
    "            self, method: Union[str, int]='any',\n",
    "            threshold: Union[int, None]=None,\n",
    "            retain_dropped_index: bool=False):\n",
    "        method = 'threshold' if threshold is not None else method\n",
    "        self.method = method\n",
    "        self.threshold = threshold\n",
    "        self.retain_dropped_index = retain_dropped_index\n",
    "    def fit(self, X: ArrayLike, y: Union[ArrayLike, None]=None) -> ArrayLike:\n",
    "        if self.retain_dropped_index:\n",
    "            if self.method == 'any':\n",
    "                self.mask = X.isna().any(axis=1).copy()\n",
    "            if self.method == 'all':\n",
    "                self.mask = X.isna().all(axis=1).copy()\n",
    "            else:\n",
    "                self.mask = (X.isna().sum(axis=1)>=threshold).copy()\n",
    "        else:\n",
    "            self.mask = None\n",
    "        return self\n",
    "    def transform(\n",
    "            self, X: ArrayLike, y: Union[ArrayLike, None]=None) -> ArrayLike:\n",
    "        if self.mask is not None:\n",
    "            X = X[~self.mask]\n",
    "        else:\n",
    "            if self.threshold is None:\n",
    "                X.dropna(how=self.method, inplace=True)\n",
    "            else:\n",
    "                X = X[~(X.isna().sum(axis=1) >= self.threshold)]\n",
    "        return X\n",
    "    def fit_transform(\n",
    "            self, X: ArrayLike, y: Union[ArrayLike, None]=None) -> ArrayLike:\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "class SelectiveKNNImputer(BaseEstimator):\n",
    "    '''A convenience class to allow use of SciKitLearn's KNNImputer to \n",
    "    impute null values in a subset of features of a given DataFrame.\n",
    "    '''\n",
    "    def __init__(\n",
    "            self, train_features: ArrayLike=[], impute_features: ArrayLike=[],\n",
    "            exclude_features: list=[], knn_imputer_args: list=[], \n",
    "            knn_imputer_kwargs: dict={}):\n",
    "        self.train_features = train_features\n",
    "        self.impute_features = impute_features\n",
    "        self.exclude_features = exclude_features\n",
    "        self.knn_imputer_args = knn_imputer_args\n",
    "        self.knn_imputer_kwargs = knn_imputer_kwargs\n",
    "        self.imputer = KNNImputer(*knn_imputer_args, **knn_imputer_kwargs)\n",
    "    def fit(self, X: ArrayLike, y: Union[ArrayLike, None]=None) -> ArrayLike:\n",
    "        if not any(self.train_features):\n",
    "            self.train_features = [\n",
    "                col for col in X.columns if col not in self.exclude_features\n",
    "            ]\n",
    "        if not any(self.impute_features):\n",
    "            self.impute_features = [\n",
    "                col for col in X.columns if col not in self.exclude_features\n",
    "            ]\n",
    "        self.imputer.fit(X[self.train_features])\n",
    "        return self\n",
    "    def transform(\n",
    "            self, X: ArrayLike, y: Union[ArrayLike, None]=None) -> ArrayLike:\n",
    "        imputed = X[self.train_features].copy()\n",
    "        imputed[self.train_features] = (\n",
    "            self.imputer.transform(imputed[self.train_features])\n",
    "        )\n",
    "        X[self.impute_features] = imputed[self.impute_features].copy()\n",
    "        del imputed\n",
    "        return X\n",
    "    def fit_transform(\n",
    "            self, X: ArrayLike, y: Union[ArrayLike, None]=None) -> ArrayLike:\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607866e2",
   "metadata": {},
   "source": [
    "Define a class to facilitate classification reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1001eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class KFCVClassificationResults:\n",
    "    name: str\n",
    "    class_results: dict[dict[str, float]]\n",
    "    accuracy: float\n",
    "    macro_avg: dict\n",
    "    weighted_avg: dict\n",
    "    \n",
    "    def report_str(\n",
    "            self, score_digits: int=4, support_digits: int=0,\n",
    "            rjust: int=18\n",
    "    )->str:\n",
    "        lbl_fmt = max([max(map(len, self.class_results.keys())) + 4, 15])\n",
    "        line_len = lbl_fmt + 4 * rjust\n",
    "        lbl_fmt = f'<{lbl_fmt}'\n",
    "        digits = 3* [f'.{score_digits}f'] + [f',.{support_digits}f']\n",
    "        score_names =  ['precision', 'recall', 'f1-score', 'support']\n",
    "        str_fmts = [f'>{rjust}{digit}' for digit in digits]\n",
    "        report = f'{\"\":{lbl_fmt}}' + ''.join(\n",
    "            f'{score_name.replace(\"-\", \" \"):>{rjust}}' for score_name in\n",
    "            score_names\n",
    "        ) + f'{BR}{hr(\"=\", lw=line_len, br=False)}'\n",
    "        for y_class in self.class_results:\n",
    "            report = report + f'{BR}{y_class:{lbl_fmt}}' + ''.join(\n",
    "                f'{self.class_results[y_class][score_name]:{fmt}}'\n",
    "                for score_name, fmt in zip(score_names, str_fmts)\n",
    "            )\n",
    "        report = report + (\n",
    "            f'{BR}-{BR}{\"accuracy\":{lbl_fmt}}' + \n",
    "            2 * f'{\"\":{rjust}}' + f'{self.accuracy:{str_fmts[0]}}'\n",
    "        )\n",
    "        for avg_name in ['macro_avg', 'weighted_avg']:\n",
    "            report = (\n",
    "                report + f'{BR}{avg_name.replace(\"_\", \" \"):{lbl_fmt}}' \n",
    "                + ''.join(\n",
    "                    f'{getattr(self, avg_name)[score_name]:{fmt}}'\n",
    "                    for score_name, fmt in zip(score_names, str_fmts)\n",
    "                )\n",
    "            )\n",
    "        del lbl_fmt, score_names, str_fmts\n",
    "        return f'{report}{BR}{hr(\"-\", lw=line_len)}-'\n",
    "def kf_classification_report(\n",
    "        X: ArrayLike, y: ArrayLike, model: BaseEstimator,\n",
    "        stratify: bool=True, kfold_args: list=[], kfold_kwargs: dict={},\n",
    "        name: str=''\n",
    ") -> dict:\n",
    "    if not name and hasattr(y, 'name'):\n",
    "        name = y.name\n",
    "    kf = {False: KFold, True: StratifiedKFold}[stratify]\n",
    "    kf = kf(*kfold_args, **kfold_kwargs)\n",
    "    y_classes = sorted([str(_) for _ in pd.unique(y)])\n",
    "    \n",
    "    report = {\n",
    "        category: {\n",
    "            metric: [] \n",
    "            for metric in [\n",
    "                'precision', 'recall', 'f1-score', 'support'\n",
    "            ]\n",
    "        } for category in [\n",
    "            *y_classes, 'macro avg', 'weighted avg'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    report['accuracy'] = []\n",
    "    for train, test in kf.split(X, y):\n",
    "        train, test = y.index[train], y.index[test]\n",
    "        X_train, X_test = X.loc[train], X.loc[test]\n",
    "        y_train, y_test = y.loc[train], y.loc[test]\n",
    "        model.fit(X_train, y_train)\n",
    "        p = pd.Series(model.predict(X_test), X_test.index)\n",
    "        cl_report = classification_report(y_test, p, output_dict=True)\n",
    "        \n",
    "        for category, details in cl_report.items():\n",
    "            if category=='accuracy':\n",
    "                report['accuracy'].append(details)\n",
    "                continue\n",
    "            for metric, value in details.items():\n",
    "                report[category][metric].append(value)\n",
    "        del X_train, X_test, y_train, y_test, p, cl_report\n",
    "    for k in [*y_classes, 'macro avg', 'weighted avg']:\n",
    "        for metric in report[k].keys():\n",
    "            report[k][metric] = np.mean(report[k][metric])\n",
    "    report['accuracy'] = np.mean(report['accuracy'])\n",
    "    report = KFCVClassificationResults(\n",
    "        name, {y_class: report[y_class] for y_class in y_classes},\n",
    "        report['accuracy'], report['macro avg'], report['weighted avg']\n",
    "    )\n",
    "    del y_classes, kf\n",
    "    return report\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
