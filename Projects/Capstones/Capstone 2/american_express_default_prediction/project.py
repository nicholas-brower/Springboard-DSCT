'''A package to facilitate the organization and presentation of files, folders,
settings, and other elements of a data science project.

Created:    2022/08/12
Modified:   2022/09/08
'''

from textwrap import fill
import pickle
import datetime
import json
from typing import Callable, Type
from dataclasses import dataclass, field, asdict as dc_asdict
from os import makedirs, path, PathLike, listdir, scandir
import numpy as np
import pandas as pd
from pprint import pprint, pformat


DATA = 'data/'
RAW = DATA + 'raw/'
WORKING = DATA + 'working/'


@dataclass
class ProjectFile:
    '''A class to facilitate organization of files and directories within a 
    project. Provides access to filename, extension, directory, and path to a 
    locally pickled version of a file used in a particular project.
    
    Attributes:
    name
        A string representing the name of the file
    filename
        The complete filename with extension
    directory
        The location of the file relative to the project directory
    stem
        The stem, base name, or "extensionless" portion of the filename
    extension
        The extension of the file
    path_to_file
        The relative path to the file as created by concatenating directory
        and filename
    path_to_pickle
        The relative path to a locally pickled version of the file
    info
        A dictionary providing a means of storing and accessing additional 
        information about a file.
    
    '''
    name: str
    filename: PathLike
    directory: PathLike
    stem: str = field(init=False)
    extension: str = field(init=False)
    path_to_file: PathLike = field(init=False)
    path_to_pickle: PathLike = field(default_factory=str)
    info: dict[str:str] = field(default_factory=dict)
    
    def __post_init__(self):
        self.path_to_file = self.directory + self.filename
        if '.' in self.filename:
            last_dot_index = self.filename.rindex('.')
            if last_dot_index != 0:
                self.extension = self.filename[last_dot_index:]
                self.stem = self.filename[:last_dot_index]
        else:
            self.extension = None
            self.stem = self.filename
        if self.extension == '.pickle' and not self.path_to_pickle:
            self.path_to_pickle = self.path_to_file
        elif self.extension == '.csv' and not self.path_to_pickle:
            self.path_to_pickle = self.directory + self.stem + '_df.pickle'
    def to_data(self, local_type: Type) -> tuple:
        '''Return arguments required to instantiate a ProjectData object for a
        given local_type.
        '''
        return (self.name, local_type, self)
    def header_to_list(self, col_sep=',', row_sep='') -> list[str]:
        '''Return a list of strings generated by splitting the first row of a
        csv file by col_sep.
        '''
        if self.extension.casefold() != '.csv':
            print('warning: file extension is not ".csv"')
            _ = input(
                'attempt to return header anyway? \nenter y/n\n'
            ).lower()
            if _ != 'y':
                return []
        with open(self.path_to_file) as csv:
            header = csv.readline()
            if row_sep:
                header = header[:header.index(row_sep)]
            header = header.rstrip('\n')
            return header.split(col_sep)

@dataclass
class PyProjectFile(ProjectFile):
    data: object = field(default_factory=object)
    filename: PathLike = field(init=False)
    log: list[str] = field(init=False)
    path_to_log: PathLike = field(init=False)
    
    def __post_init__(self):
        self.stem = self.name
        self.extension = '.pickle'
        self.filename = f'{self.name}{self.extension}'
        self.path_to_file = f'{self.directory}{self.filename}'
        self.path_to_pickle = self.path_to_file
        self.path_to_log = f'{self.directory}{self.stem}_log'
        if path.exists(self.path_to_file):
            with open(self.path_to_file, 'rb') as file:
                self.data = pickle.load(file)
        else:
            self.data = {}
        if path.exists(self.path_to_log):
            with open(self.path_to_log, 'rb') as file:
                self.log = pickle.load(file)
        else:
            self.log = []
        
    def add_to_log(self, action: str) -> None:
        self.log.append(action)
        with open(self.path_to_log, 'wb') as file:
            pickle.dump(self.log, file)
    def save(self) -> None:
        with open(self.path_to_file, 'wb') as file:
            pickle.dump(self.data, file)
        
        

@dataclass
class ProjectData:
    '''A class to access, import, track, convert, and store tabular data 
    objects used in a project.
    '''
    name: str
    local_type: Type
    source: ProjectFile
    source_path: PathLike = field(init=False)
    
    def __post_init__(self):
        if path.exists(self.source.path_to_pickle):
            self.source_path = self.source.path_to_pickle
        else:
            self.source_path = self.source.path_to_file
    
    def load(self) -> pd.DataFrame:
        if self.local_type == pd.DataFrame:
            if self.source_path == self.source.path_to_pickle:
                return pd.read_pickle(self.source_path)
            elif self.source.extension == '.csv':
                pd.read_csv(self.source_path).to_pickle(self.source.path_to_pickle)
                self.source = ProjectFile(
                    self.source.stem + ' pickle', 
                    self.source.stem + '_df.pickle',
                    self.source.directory
                )
                self.source_path = self.source.path_to_pickle
                return pd.read_pickle(self.source_path)

def csv_header_to_list(csv_path: PathLike, delimiter: str=',') -> list[str]:
    '''Return the header of a csv file, split by delimiter, as a list of
    strings.
    '''
    with open(csv_path) as csv:
        header = csv.readline()
    header = header.split(delimiter)
    return header
def hr(divider: str='=', width:int=79, newline:bool=True) -> str:
    '''Return a horizontal rule. Functionally similar to html's <hr>.
    
    Arguments:
    divider
        a string to repeat
    width
        An integer representing the total width in characters of the resultant
        horizontal rule. Default = 79, the maximum recommended line width as 
        specified in PEP 8.
    newline
        If True, add a newline character to the end of the returned string. 
        Default = True.
        
    Returns:
        A string of repeated divider having len(string) = width.
    '''
    return f'{"":{divider}>{width}}' + int(newline)*'\n'
def ffill(text: str, width: int=79, num_tabs: int=0, tab_w: int=4) -> str:
    '''Wrap text to line widths of a specified value. Default line width is as 
    specified in PEP 8 (79 characters). Apply initial and subsequent indents
    by number of tabs, with one tab = tab_w spaces.
    
    Arguments
    text
        a string to wrap
    width
        An integer specifying maximum linewidth. Default = 79.
    num_tabs
        An integer specifying the number of tabs used to indent each line in
        the resultant string. Default = 0.
    tab_w
        An integer specifying the width in spaces of each tab. Default = 4.
    
    Returns
        A string as described above.
    '''
    indent = num_tabs * tab_w * ' '
    return fill(
        width=width, text=text, initial_indent=indent, 
        subsequent_indent=indent, break_long_words=False, 
        break_on_hyphens=False, expand_tabs=False
    )
def print_table(
        tabular_data: list[str], divider: str = '|', 
        spacing: int=2, lw: int=79, title: str='') -> None:
    '''Print a list of strings as a table.
    '''
    max_len = max([len(f'{_}') for _ in tabular_data])
    tabular_data = [f'{_}'.ljust(max_len) for _ in tabular_data]
    divider = f'{spacing* " "}{divider}{spacing * " "}'
    rows = []
    row = []
    c = 0
    for i in range(len(tabular_data)):
        if not row:
            row.append(tabular_data[i])
            c += 1
        elif len(''.join(row)) + len(divider) + len(tabular_data[i]) < lw:
            row.extend([divider, tabular_data[i]])
            c += 1
        else:
            rows.append(''.join(row))
            row = []
            c = 0
    if c:
        rows.append(''.join(row))
    if title:
        print(f'{title}\n{hr("-", len(rows[0]), newline=False)}')
    print('\n'.join(rows))
def check_git_ignore() -> None:
    '''Check for a .gitignore file in the project directory. If one does
    not exist, create one.'''
    to_ignore = [
        f'\n{_}' for _ in [
            '*.bak', '*.pickle', '*.pkl', '**/pickle/', 
            '.ipynb_checkpoints/', '_pycache_', '*connection*',
            '*api*key*', '.env', 'env'
            ]
    ]
    to_ignore.extend([_.upper() for _ in to_ignore])
    if not path.exists('.gitignore'):
        with open('.gitignore', 'a') as gitignore:
            gitignore.writelines(to_ignore)
    else:
        with open('.gitignore', 'r') as gitignore:
            ignored = gitignore.read()
        with open('.gitignore', 'a') as gitignore:
            for item in to_ignore:
                if item.strip('\n') not in ignored:
                    gitignore.write(item)
def git_ignore(s: str) -> None:
    '''If s is not already in the project's .gitignore file, add it.
    '''
    with open('.gitignore', 'r') as gitignore:
        ignored = gitignore.readlines()
        if any([line==s for line in ignored]):
            return
    with open('.gitignore', 'a') as gitignore:
        gitignore.write(f'\n{s}')
def inspect_csv(csv_file: ProjectFile) -> None:
    '''Inspect lines of a csv file. Print header, suspected number of
    columns, a sample of row data, and the names and formats of columns that
    likely contain datetimes.
    
    Arguments:
    csv_file
        A ProjectFile providing the name and path of a csv file.
    
    Returns:
        None. Information is printed.
    '''
    with open(csv_file.path_to_file) as csv:
        header = csv.readline().split(',')
        dots = '' + int(len(header) > 10) * '...'
        print(f'{csv_file.name}:\n' + hr(newline=False))
        print(f'relative path: {csv_file.path_to_file}')
        print(f'{len(header)} columns\n')
        print('header:   ' + (', '.join(header[:10]) + dots) + '\n')
        row_1 = csv.readline().split(',') + int(len(header)>10) * ['...']
        date_cols = []
        for i, item in enumerate(row_1):
            try:
                _ = np.datetime64(item)
                if not np.isnat(_):
                    date_cols.append((_, header[i]))
            except:
                continue
        if date_cols:
            print(f'possible date columns: \n' + ffill(', '.join(
                f'{_[1]} (inferred from "{_[0]}")' for _ in date_cols
            ), num_tabs=1) + '\n')
    print(
        'row 1:' + int(len(header) > 10) * 
        ' (10 elements; elements limited to 10 characters)' + '\n' 
          + hr('-') + ffill(', '.join(f'{__}'[:10] for __ in row_1[:10]))
    )
def load_and_pickle_csv(csv_file: ProjectFile) -> pd.DataFrame:
    '''Load a csv file as a DataFrame if it has not already been stored as a
    pickle file. Save this DataFrame as a pickle file. Return a DataFrame
    loaded from a pickle file.
    
    Arguments:
    csv_file
        A ProjectFile providing the name and path of a csv file, and the path
        of a pickled version of a DataFrame created from this csv file. If the
        pickle file does not exist, it is created.
    
    Returns
        pd.DataFrame
    '''
    if not path.exists(project_file.path_to_pickle):
        df = pd.read_csv(project_file.path_to_file)
        df.to_pickle(project_file.path_to_pickle)
    df = pd.read_pickle(project_file.path_to_pickle)
    return df

def df_summary(df: pd.DataFrame, id_col: str='', datetime_col: str='') -> str:
    '''Return basic information about a DataFrame object.
    '''
    summary = {
        'total records': f'{df.shape[0]:,}',
        'total columns': f'{df.shape[1]:,}',
        'null percent': f'{100 * df.isna().mean().mean():.2f}%'
    }
    if id_col:
        summary['total invididuals'] = (
            f'{len(df[id_col].dropna().unique()):,}')
    if datetime_col:
        summary[ 'start date'] = f'{df[datetime_col].min()}'
        summary['end date'] = f'{df[datetime_col].max()}'
    if df.attrs.get('name', None):
        print(f'{df.attrs["name"]}\n{hr(newline=False)}')
    summary = '\n'.join(
        f'{name}'.ljust(20) + f'{value}'.rjust(20)
        for name, value in summary.items()
    ) + '\n'
    return summary
def df_max_str_lens(df: pd.DataFrame) -> str:
    col_lens = {}
    for col in df.columns:
        if pd.api.types.is_object_dtype(df[col]):
            col_lens[col] = df[col].astype(str).str.len().max()
    return '\n'.join([
        f'{col}:'.ljust(15) + f'{length}'
        for col, length in col_lens.items()
    ])
            
        
        
def print_feature_summary(feature_d: dict) -> None:
    for feature, details in train_features.data.items():
        print(
            f'{feature}\n{hr("_")}category:{details["category"]}\n'
            + f'feature type: {details["f_type"]}\ndata type: '
            + f'{details["d_type"].__name__}\nnull percent: '
            + f'{100 * details["null percent"]:.2f}\n{hr(".")}'
        )
        if 'null correlation with target' in details:
            print('isnull correlation with target:    '
                   + f'{details["null correlation with target"]:.3f}\n')
        if 'flags' in details:
            print('    *' + '\n    *'.join(details['flags']) + '\n')
        if details['f_type'] == 'categorical':
            for subfeature in details['subfeatures']:
                print(f'    subfeature {subfeature}\n    {hr("-", 75)}')
                print('    summary statistics:')
                print(
                    '\n'.join(f'    {line}' for line in pformat(
                        details['subfeatures'][subfeature]['summary'], width=75
                    ).split('\n')) + '\n'
                )
        else:
            print('summary statistics:\n')
            overall = details['summary']['unweighted']['overall']
            k = list(overall.keys())
            overall = list(overall.values())
            by_target = details['summary']['unweighted']['by target']
            defs = [value for key, value in by_target['default'].items() if key in k]
            no_defs = [value for key, value in by_target['no default'].items() if key in k]
            v = [[no_defs[i], defs[i], overall[i]] for i in range(len(k))]
            v = '\n'.join([
                k[i].ljust(6) + ''.join(f'{_:.4f}'.rjust(14) for _ in v[i]) 
                for i in range(len(v))
            ])
            print(6*' ' + ''.join(f'{_}'.rjust(14) for _ in ['no default', 'default', 'overall']))
            print(f'{v}\n')
            print('target correlation by aggregate:\n')
            target_corr = sorted([
                [key, value] for key, value 
                in details['target correlation by aggregate'].items()
            ], key=lambda t: abs(t[1]), reverse=True)
            for aggregate, value in target_corr:
                print(f'{aggregate}:'.ljust(12) + f'{value:.4f}'.rjust(8))
            print(39*'. '+'\n\n')
    
    


for folder in DATA, RAW, WORKING:
    if not path.exists(folder):
        makedirs(folder)

check_git_ignore()

